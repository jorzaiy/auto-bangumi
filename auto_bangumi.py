import feedparser
import requests
import json
import os
import re
import argparse
import hashlib
import time
import glob
import shutil
from datetime import datetime
from urllib.parse import urlparse, parse_qs, quote
from dotenv import load_dotenv

# å°è¯•å¯¼å…¥ curl_cffi ç”¨äºç»•è¿‡åçˆ¬è™«
try:
    from curl_cffi import requests as cffi_requests
    HAS_CURL_CFFI = True
except ImportError:
    HAS_CURL_CFFI = False
    print("æç¤º: å®‰è£… curl_cffi å¯ä»¥ç»•è¿‡ mikanani.tv çš„åçˆ¬è™« (pip install curl_cffi)")

# åŠ è½½ .env æ–‡ä»¶
load_dotenv()

# --- é…ç½®éƒ¨åˆ† ---
# Alist é…ç½®
ALIST_HOST = os.getenv("ALIST_HOST", "http://127.0.0.1:5244")
ALIST_TOKEN = os.getenv("ALIST_TOKEN", "")
TARGET_PATH = os.getenv("TARGET_PATH", "/Anime")

# Aria2 é…ç½®
ARIA2_HOST = os.getenv("ARIA2_HOST", "http://localhost:6800/jsonrpc")
ARIA2_SECRET = os.getenv("ARIA2_SECRET", "")
DOWNLOAD_DIR = os.getenv("DOWNLOAD_DIR", "/root/downloads")

# æ•°æ®æ–‡ä»¶
SUBSCRIPTIONS_FILE = os.getenv("SUBSCRIPTIONS_FILE", "subscriptions.json")
HISTORY_FILE = os.getenv("HISTORY_FILE", "downloaded.json")

# æ­£åˆ™è¿‡æ»¤ (å¯é€‰ï¼Œæ¯”å¦‚åªä¸‹ 1080p)
FILTER_REGEX = os.getenv("FILTER_REGEX", r"1080[pP]")

# --- è®¢é˜…ç®¡ç† ---

def load_subscriptions():
    if not os.path.exists(SUBSCRIPTIONS_FILE):
        return []
    with open(SUBSCRIPTIONS_FILE, 'r', encoding='utf-8') as f:
        return json.load(f)

def save_subscriptions(subs):
    with open(SUBSCRIPTIONS_FILE, 'w', encoding='utf-8') as f:
        json.dump(subs, f, ensure_ascii=False, indent=2)

def parse_mikan_url(url):
    parsed = urlparse(url)
    params = parse_qs(parsed.query)
    bangumi_id = params.get('bangumiId', [''])[0]
    subgroup_id = params.get('subgroupid', [''])[0]
    return bangumi_id, subgroup_id

def get_next_id(subs):
    if not subs:
        return 1
    return max(s['id'] for s in subs) + 1

def find_subscription(subs, identifier):
    try:
        sub_id = int(identifier)
        for sub in subs:
            if sub['id'] == sub_id:
                return sub
    except ValueError:
        pass
    for sub in subs:
        if sub['name'] == identifier:
            return sub
    return None

def add_subscription(url, name=None):
    subs = load_subscriptions()
    bangumi_id, subgroup_id = parse_mikan_url(url)
    for sub in subs:
        if sub['url'] == url:
            print(f"è®¢é˜…å·²å­˜åœ¨: #{sub['id']} {sub['name']}")
            return None
    new_sub = {
        'id': get_next_id(subs),
        'name': name or f"è®¢é˜…_{bangumi_id}",
        'url': url,
        'bangumi_id': bangumi_id,
        'subgroup_id': subgroup_id,
        'enabled': True,
        'added_at': datetime.now().isoformat()
    }
    subs.append(new_sub)
    save_subscriptions(subs)
    print(f"å·²æ·»åŠ è®¢é˜… #{new_sub['id']}: {new_sub['name']}")
    return new_sub

def remove_subscription(identifier):
    subs = load_subscriptions()
    sub = find_subscription(subs, identifier)
    if not sub:
        print(f"æœªæ‰¾åˆ°è®¢é˜…: {identifier}")
        return False
    subs.remove(sub)
    save_subscriptions(subs)
    print(f"å·²åˆ é™¤è®¢é˜…: #{sub['id']} {sub['name']}")
    return True

def update_subscription(identifier, name=None, url=None, enabled=None):
    subs = load_subscriptions()
    sub = find_subscription(subs, identifier)
    if not sub:
        print(f"æœªæ‰¾åˆ°è®¢é˜…: {identifier}")
        return False
    if name is not None:
        sub['name'] = name
    if url is not None:
        sub['url'] = url
        sub['bangumi_id'], sub['subgroup_id'] = parse_mikan_url(url)
    if enabled is not None:
        sub['enabled'] = enabled
    save_subscriptions(subs)
    status = "å¯ç”¨" if sub['enabled'] else "ç¦ç”¨"
    print(f"å·²æ›´æ–°è®¢é˜… #{sub['id']}: {sub['name']} [{status}]")
    return True

def list_subscriptions():
    subs = load_subscriptions()
    if not subs:
        print("æš‚æ— è®¢é˜…ï¼Œä½¿ç”¨ add å‘½ä»¤æ·»åŠ ")
        return
    print(f"{'ID':<4} {'åç§°':<20} {'çŠ¶æ€':<6} {'bangumiId':<10} {'subgroupId':<10}")
    print("-" * 60)
    for sub in subs:
        status = "å¯ç”¨" if sub['enabled'] else "ç¦ç”¨"
        print(f"{sub['id']:<4} {sub['name']:<20} {status:<6} {sub['bangumi_id']:<10} {sub['subgroup_id']:<10}")

# --- å†å²è®°å½•ç®¡ç† ---

def load_history():
    if not os.path.exists(HISTORY_FILE):
        return []
    with open(HISTORY_FILE, 'r', encoding='utf-8') as f:
        return json.load(f)

def save_history(history):
    with open(HISTORY_FILE, 'w', encoding='utf-8') as f:
        json.dump(history, f, ensure_ascii=False)

# --- Torrent è½¬ç£åŠ›é“¾æ¥ ---

def bdecode(data):
    def decode_next(data, idx):
        char = chr(data[idx])
        if char == 'i':
            end = data.index(b'e', idx)
            return int(data[idx+1:end]), end + 1
        elif char == 'l':
            idx += 1
            result = []
            while chr(data[idx]) != 'e':
                val, idx = decode_next(data, idx)
                result.append(val)
            return result, idx + 1
        elif char == 'd':
            idx += 1
            result = {}
            while chr(data[idx]) != 'e':
                key, idx = decode_next(data, idx)
                if isinstance(key, bytes):
                    key = key.decode('utf-8', errors='replace')
                val, idx = decode_next(data, idx)
                result[key] = val
            return result, idx + 1
        elif char.isdigit():
            colon = data.index(b':', idx)
            length = int(data[idx:colon])
            start = colon + 1
            return data[start:start+length], start + length
        else:
            raise ValueError(f"Invalid bencode at {idx}")
    result, _ = decode_next(data, 0)
    return result

def bencode(data):
    if isinstance(data, int):
        return f'i{data}e'.encode()
    elif isinstance(data, bytes):
        return f'{len(data)}:'.encode() + data
    elif isinstance(data, str):
        encoded = data.encode('utf-8')
        return f'{len(encoded)}:'.encode() + encoded
    elif isinstance(data, list):
        return b'l' + b''.join(bencode(item) for item in data) + b'e'
    elif isinstance(data, dict):
        items = sorted(data.items())
        return b'd' + b''.join(bencode(k) + bencode(v) for k, v in items) + b'e'
    else:
        raise TypeError(f"Cannot bencode {type(data)}")

def torrent_to_magnet(torrent_url):
    try:
        if HAS_CURL_CFFI:
            resp = cffi_requests.get(torrent_url, impersonate="chrome", timeout=30)
        else:
            resp = requests.get(torrent_url, timeout=30)
        resp.raise_for_status()
        torrent_data = bdecode(resp.content)
        info = torrent_data.get('info', {})
        info_encoded = bencode(info)
        info_hash = hashlib.sha1(info_encoded).hexdigest()
        name = info.get('name', b'')
        if isinstance(name, bytes):
            name = name.decode('utf-8', errors='replace')
        magnet = f"magnet:?xt=urn:btih:{info_hash}"
        if name:
            magnet += f"&dn={quote(name)}"
        if 'announce' in torrent_data:
            announce = torrent_data['announce']
            if isinstance(announce, bytes):
                announce = announce.decode('utf-8', errors='replace')
            magnet += f"&tr={quote(announce)}"
        return magnet
    except Exception as e:
        print(f"  è½¬æ¢ç£é“¾å¤±è´¥: {e}")
        return None

# --- RSS è·å– (ç»•è¿‡åçˆ¬è™«) ---

def fetch_rss(url):
    if HAS_CURL_CFFI:
        try:
            resp = cffi_requests.get(url, impersonate="chrome", timeout=30)
            return feedparser.parse(resp.text)
        except Exception as e:
            print(f"  curl_cffi è·å–å¤±è´¥: {e}ï¼Œå°è¯•æ™®é€šæ–¹å¼")
    return feedparser.parse(url)

# --- Aria2 RPC ---

def aria2_rpc(method, params=None):
    payload = {
        "jsonrpc": "2.0",
        "id": "auto_bangumi",
        "method": method,
        "params": params or []
    }
    if ARIA2_SECRET:
        if params:
            payload["params"] = [f"token:{ARIA2_SECRET}"] + list(params)
        else:
            payload["params"] = [f"token:{ARIA2_SECRET}"]
    try:
        resp = requests.post(ARIA2_HOST, json=payload, timeout=10)
        return resp.json()
    except Exception as e:
        print(f"  Aria2 RPC é”™è¯¯: {e}")
        return None

def add_to_aria2(uri, filename):
    options = {"dir": DOWNLOAD_DIR}
    result = aria2_rpc("aria2.addUri", [[uri], options])
    if result and "result" in result:
        gid = result["result"]
        print(f"âœ… å·²æ·»åŠ åˆ° Aria2: {filename} (GID: {gid})")
        return gid
    else:
        print(f"âŒ æ·»åŠ åˆ° Aria2 å¤±è´¥: {result}")
        return None

def get_aria2_status(gid):
    result = aria2_rpc("aria2.tellStatus", [gid])
    if result and "result" in result:
        return result["result"]
    return None

def get_aria2_downloading_files():
    """è·å– Aria2 ä¸­æ­£åœ¨ä¸‹è½½å’Œç­‰å¾…ä¸­çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨"""
    downloading_files = set()
    # è·å–æ­£åœ¨ä¸‹è½½çš„ä»»åŠ¡
    active = aria2_rpc("aria2.tellActive", [["files"]])
    if active and "result" in active:
        for task in active["result"]:
            for f in task.get("files", []):
                path = f.get("path", "")
                if path:
                    downloading_files.add(path)
    # è·å–ç­‰å¾…ä¸­çš„ä»»åŠ¡
    waiting = aria2_rpc("aria2.tellWaiting", [0, 100, ["files"]])
    if waiting and "result" in waiting:
        for task in waiting["result"]:
            for f in task.get("files", []):
                path = f.get("path", "")
                if path:
                    downloading_files.add(path)
    return downloading_files

# --- ä¸Šä¼ åˆ°å¤¸å…‹ ---

def upload_to_alist(local_path, remote_path):
    url = f"{ALIST_HOST}/api/fs/put"
    file_size = os.path.getsize(local_path)
    headers = {
        "Authorization": ALIST_TOKEN,
        "File-Path": quote(remote_path, safe=''),
        "Content-Length": str(file_size),
    }
    # æ ¹æ®æ–‡ä»¶å¤§å°åŠ¨æ€è°ƒæ•´è¶…æ—¶æ—¶é—´ï¼ˆæ¯ 100MB å¢åŠ  60 ç§’ï¼‰
    timeout = max(300, (file_size // (100 * 1024 * 1024)) * 60 + 300)
    try:
        with open(local_path, 'rb') as f:
            resp = requests.put(url, headers=headers, data=f, timeout=timeout)
            res_data = resp.json()
            if res_data.get('code') == 200:
                print(f"âœ… ä¸Šä¼ æˆåŠŸ: {remote_path}")
                return True
            else:
                print(f"âŒ ä¸Šä¼ å¤±è´¥: {res_data}")
                return False
    except Exception as e:
        print(f"âš ï¸ ä¸Šä¼ é”™è¯¯: {e}")
        return False

def process_completed_downloads():
    if not os.path.exists(DOWNLOAD_DIR):
        return
    # è·å– Aria2 ä¸­æ­£åœ¨ä¸‹è½½çš„æ–‡ä»¶ï¼Œé¿å…ä¸Šä¼ ä¸å®Œæ•´çš„æ–‡ä»¶
    downloading_files = get_aria2_downloading_files()
    files = glob.glob(os.path.join(DOWNLOAD_DIR, "*"))
    for filepath in files:
        if filepath.endswith(".aria2"):
            continue
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æ­£åœ¨ä¸‹è½½ä¸­
        if filepath in downloading_files:
            print(f"â³ è·³è¿‡ (ä¸‹è½½ä¸­): {os.path.basename(filepath)}")
            continue
        filename = os.path.basename(filepath)
        remote_path = f"{TARGET_PATH}/{filename}"
        print(f"æ­£åœ¨ä¸Šä¼ : {filename}")
        if upload_to_alist(filepath, remote_path):
            try:
                if os.path.isfile(filepath):
                    os.remove(filepath)
                elif os.path.isdir(filepath):
                    shutil.rmtree(filepath)
                print(f"ğŸ—‘ï¸ å·²æ¸…ç†æœ¬åœ°æ–‡ä»¶: {filename}")
            except Exception as e:
                print(f"âš ï¸ æ¸…ç†å¤±è´¥: {e}")

# --- ä¸»é€»è¾‘ ---

def check_single_subscription(sub, history):
    print(f"\næ£€æŸ¥è®¢é˜…: {sub['name']}")
    feed = fetch_rss(sub['url'])
    if not feed.entries:
        print(f"  è­¦å‘Š: æœªè·å–åˆ°ä»»ä½•æ¡ç›®ï¼Œå¯èƒ½ RSS è·å–å¤±è´¥")
        return []
    new_items = []
    for entry in reversed(feed.entries):
        title = entry.title
        guid = entry.get('guid', entry.get('id', entry.link))
        if guid in history:
            continue
        if FILTER_REGEX and not re.search(FILTER_REGEX, title):
            print(f"  è·³è¿‡ (ä¸åŒ¹é…è§„åˆ™): {title}")
            continue
        print(f"  å‘ç°æ–°ç•ªå‰§: {title}")
        magnet_link = None
        if hasattr(entry, 'enclosures') and entry.enclosures:
            torrent_url = entry.enclosures[0].get('href', '')
            if torrent_url:
                print(f"  è½¬æ¢ç£é“¾ä¸­...")
                magnet_link = torrent_to_magnet(torrent_url)
                if magnet_link:
                    print(f"  ç£é“¾: {magnet_link[:60]}...")
        if magnet_link and add_to_aria2(magnet_link, title):
            new_items.append(guid)
        elif not magnet_link:
            print(f"  è·³è¿‡ (æ— æ³•è·å–ç£é“¾)")
    return new_items

def run_check():
    subs = load_subscriptions()
    enabled_subs = [s for s in subs if s['enabled']]
    if not enabled_subs:
        print("æš‚æ— å¯ç”¨çš„è®¢é˜…ï¼Œä½¿ç”¨ add å‘½ä»¤æ·»åŠ ")
        return
    print(f"å¼€å§‹æ£€æŸ¥ RSS æ›´æ–°... (å…± {len(enabled_subs)} ä¸ªè®¢é˜…)")
    history = load_history()
    new_history = history.copy()
    for i, sub in enumerate(enabled_subs, 1):
        print(f"\n[{i}/{len(enabled_subs)}]", end="")
        new_items = check_single_subscription(sub, history)
        new_history.extend(new_items)
    save_history(new_history)
    print("\næ£€æŸ¥å®Œæˆ!")

def run_upload():
    print("æ£€æŸ¥å·²å®Œæˆçš„ä¸‹è½½...")
    process_completed_downloads()
    print("ä¸Šä¼ å¤„ç†å®Œæˆ!")

# --- CLI å…¥å£ ---

def main():
    parser = argparse.ArgumentParser(description='Mikan ç•ªå‰§ RSS è®¢é˜…ç®¡ç†ä¸è‡ªåŠ¨ä¸‹è½½å·¥å…·')
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')
    add_parser = subparsers.add_parser('add', help='æ·»åŠ æ–°è®¢é˜…')
    add_parser.add_argument('url', help='RSS è®¢é˜…åœ°å€')
    add_parser.add_argument('--name', '-n', help='è®¢é˜…åç§° (å¯é€‰)')
    subparsers.add_parser('list', help='åˆ—å‡ºæ‰€æœ‰è®¢é˜…')
    remove_parser = subparsers.add_parser('remove', help='åˆ é™¤è®¢é˜…')
    remove_parser.add_argument('identifier', help='è®¢é˜… ID æˆ–åç§°')
    update_parser = subparsers.add_parser('update', help='æ›´æ–°è®¢é˜…')
    update_parser.add_argument('identifier', help='è®¢é˜… ID æˆ–åç§°')
    update_parser.add_argument('--name', '-n', help='æ–°åç§°')
    update_parser.add_argument('--url', '-u', help='æ–° URL')
    update_parser.add_argument('--enable', action='store_true', help='å¯ç”¨è®¢é˜…')
    update_parser.add_argument('--disable', action='store_true', help='ç¦ç”¨è®¢é˜…')
    subparsers.add_parser('run', help='è¿è¡Œä¸‹è½½æ£€æŸ¥')
    subparsers.add_parser('upload', help='ä¸Šä¼ å·²ä¸‹è½½çš„æ–‡ä»¶åˆ°å¤¸å…‹')
    args = parser.parse_args()
    if args.command == 'add':
        add_subscription(args.url, args.name)
    elif args.command == 'list':
        list_subscriptions()
    elif args.command == 'remove':
        remove_subscription(args.identifier)
    elif args.command == 'update':
        enabled = None
        if args.enable:
            enabled = True
        elif args.disable:
            enabled = False
        update_subscription(args.identifier, name=args.name, url=args.url, enabled=enabled)
    elif args.command == 'run' or args.command is None:
        run_check()
    elif args.command == 'upload':
        run_upload()
    else:
        parser.print_help()

if __name__ == "__main__":
    main()
